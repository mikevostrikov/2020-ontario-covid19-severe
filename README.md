# Ontario COVID-19 Severe Cases Analysis
### Background
It is common to see the _reported number of people tested positive_ as an ultimate metric of the spread of the COVID-19. The metric's problem is that many factors significantly impact the metric's value, and these factors can change arbitrarily over time.

Some of these factors are:
- most people are not getting tested, and the number of people getting tested
- policies regarding who is eligible or required to be tested
- the availability of the tests
- the quality of tests

That is why the metric is not the best for monitoring the disease's spread over long periods of time (e.g. longer than two or three weeks).

For a historical analysis, I consider metrics _newly hospitalized_ and _newly accepted to ICU_ to be better than the _reported number of people tested positive_, because there are fewer arbitrarily changing factors in play. Indeed, the percent of people requiring hospitalization or ICU is defined by the disease itself and by existing medical protocols, which do not change often.

Ontario provides a fantastic [data tool](https://covid-19.ontario.ca/data) with various interactive graphs that help us to explore the current situation with COVID-19 in the province. Two extensive datasets are available in CSV format on the website. Unfortunately, neither the graphs nor the datasets contain _newly hospitalized_ or _newly accepted to ICU_ metrics. The closest available metrics are numbers of currently hospitalized people and people in the ICU and daily changes in these numbers.

I found that the province publishes [daily reports](https://covid-19.ontario.ca/covid-19-epidemiologic-summaries-public-health-ontario) containing metrics _ever hospitalized_ and _ever in ICU_. We could derive the desired _newly hospitalized_ and _newly accepted to ICU_ numbers from these reports. The only inconvenience is that these reports are in PDF format.

### Goals of the project
This project aimed to:
- derive _newly hospitalized_ and _newly accepted to ICU_ metrics from the available PDF reports
- allow people to analyze the metrics by publishing a historical dataset in CSV format
- enable daily monitoring of the metrics by providing respective graphs

You can observe the results of the project on the [website](https://covid19.mikevostrikov.com).

### How it works
The solution is straightforward. There are three parts to it:
- ETL data pipeline
- its operation
- website

##### ETL data pipeline
The pipeline is implemented as a Python script. The script does the following:
1. Parses the [daily reports](https://covid-19.ontario.ca/covid-19-epidemiologic-summaries-public-health-ontario) page to extract links to PDF-reports
1. Extracts the metrics from PDF-reports using [pdfminer.six](https://pypi.org/project/pdfminer.six) library and RegEx
1. Combines existing historical dataset with newly extracted metrics and overwrites the dataset with the result using [Pandas](https://pypi.org/project/pandas/) library
1. Generates embeddable interactive HTML/js graphs of the historical data using [plotly](https://pypi.org/project/plotly/) library
1. Publishes historical dataset and the graphs to the AWS S3 bucket that hosts the [website](https://covid19.mikevostrikov.com)

You can find the script and more detailed documentation in the Jupyter notebook `/etl/pipeline.ipynb`.

##### ETL data pipeline operation
The data pipeline Python script is wrapped in an AWS lambda function. The AWS EventBridge service runs the lambda function on a daily schedule.
The code of the lambda function is available in the `/etl/lambda_function.py`.
###### Deployment
The deployment of the lambda function is semi-automatic. The deployment script is available in the file `/helper/aws-lambda-deploy`. The script should be run on a clean AWS EC2 Linux AMI 2 instance. The script performs the following steps:
1. Install Python and git and clone the current repo
1. Create Python virtual environment using [virtualenv](https://pypi.org/project/virtualenv/) library
1. Install all the Python libraries according to the `/etl/requirements.txt`
1. Convert Jupyter notebook into a plain script using the script `/helper/ipynb_to_py.py`
1. Package all the dependencies, the pipeline script and the lambda function into a lambda function zip-archive
1. Move the archive to the AWS S3 bucket, and deploy the lambda function from there

##### Website
The [website](https://covid19.mikevostrikov.com) is hosted from AWS S3 bucket via AWS CloudFront service.
There is a single simple page `/web/index.html` that:
1. Contains general information about the published data
1. Displays the graphs generated by the ETL pipeline
1. Shows the historical data generated by the ETL pipeline in a form of a table

###### Tech
- [jQuery](https://jquery.com/) - performs http-requests, html manipulation
- [Plotly](https://plotly.com/javascript/) - enables the graphing
